{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sc\n",
    "sc.init('subscriber_searching.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.sc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Crawler (Scraping the Web)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Wiki - Web Crawler](https://www.wikiwand.com/en/Web_crawler)\n",
    "======\n",
    "> A Web crawler is an Internet bot which **systematically browses** the World Wide Web, typically for the purpose of Web indexing\n",
    "\n",
    "![crawler arch](images/WebCrawlerArchitecture.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video:[Web Crawler - CS101 - Udacity](https://www.youtube.com/watch?v=CDXOcvUNBaA&hd=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where to start? \n",
    "* [Top 30 Free Web Scraping Software](http://www.octoparse.com/blog/top-30-free-web-scraping-software/)\n",
    "- [八爪鱼采集器](http://www.bazhuayu.com/about)\n",
    "- [Scrapy](http://scrapy.org/)  Demo:⬇︎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dmoz_spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "class DmozSpider(scrapy.Spider):\n",
    "    name = \"dmoz\"\n",
    "    allowed_domains = [\"dmoz.org\"]\n",
    "    start_urls = [\n",
    "        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Books/\",\n",
    "        \"http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        filename = response.url.split(\"/\")[-2] + '.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scrapy crawl dmoz\n",
    "\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Scrapy started (bot: tutorial)\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Optional features available: ...\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Overridden settings: {}\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled extensions: ...\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled downloader middlewares: ...\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled spider middlewares: ...\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Enabled item pipelines: ...\n",
    "2014-01-23 18:13:07-0400 [scrapy] INFO: Spider opened\n",
    "2014-01-23 18:13:08-0400 [scrapy] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Resources/> (referer: None)\n",
    "2014-01-23 18:13:09-0400 [scrapy] DEBUG: Crawled (200) <GET http://www.dmoz.org/Computers/Programming/Languages/Python/Books/> (referer: None)\n",
    "2014-01-23 18:13:09-0400 [scrapy] INFO: Closing spider (finished)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " [Wiki - Web Scraping](https://www.wikiwand.com/en/Web_scraping)\n",
    " ======\n",
    "> Web scraping (web harvesting or web data extraction) is a computer software technique of extracting information from websites.\n",
    "\n",
    "> Techniques:\n",
    "> - Human copy-and-paste\n",
    "> - Text grepping and regular expression matching  \n",
    "[regular expression for url](http://t.cn/RcrMHbr)\n",
    "> - HTTP programming\n",
    "> - HTML parsers\n",
    "![HTML Tags](images/html_tags.png)\n",
    "(https://www.nobledesktop.com/html-quick-guide/)\n",
    "> - DOM(_Document Object Model_) parsing\n",
    "![DOM Tree](images/domTree.jpg)\n",
    "> - Web-scraping software\n",
    "> - Vertical aggregation platforms\n",
    "> - Semantic annotation recognizing\n",
    "> - Computer vision web-page analyzers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [《Web Scraping with Python: Collecting Data from the Modern Web》](http://shop.oreilly.com/product/0636920034391.do)\n",
    "#### by Ryan Mitchell (2015) _4.5 out of 5 stars_\n",
    "\n",
    "![book logo](images/51ZuvPdvCjL._SX379_BO1,204,203,200_.jpg)\n",
    "\n",
    "> Learn web scraping and crawling techniques to access unlimited data from any web source in any format.\n",
    "> - Learn how to __parse__ complicated HTML pages\n",
    "> - __Traverse__ multiple pages and sites\n",
    "> - Get a general overview of __APIs__ and how they work\n",
    "> - Learn several methods for __storing__ the data you scrape\n",
    "> - Download, read, and __extract__ data from documents\n",
    "> - Use tools and techniques to __clean__ badly formatted data\n",
    "> - Read and write __natural languages__\n",
    "> - Crawl through __forms and logins__\n",
    "> - Understand how to scrape __JavaScript__\n",
    "> - Learn __image processing and text recognition__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Scrapy](http://scrapy.org/)'s Features\n",
    "> - Write script in Python\n",
    "> - Powerful WebUI with script editor, task monitor, project manager and result viewer\n",
    "> - MySQL, MongoDB, Redis, SQLite, PostgreSQL with SQLAlchemy as database backend\n",
    "> - RabbitMQ, Beanstalk, Redis and Kombu as message queue\n",
    "> - Task priority, retry, periodical, recrawl by age, etc...\n",
    "> - Distributed architecture, Crawl Javascript pages, Python 2&3, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There's libs for that!\n",
    "\n",
    "* [pattern](http://www.clips.ua.ac.be/pages/pattern)\n",
    "* [lxml](http://lxml.de/)\n",
    "* [requests](http://docs.python-requests.org/en/latest/)\n",
    "* [Scrapy](https://scrapy.org/)\n",
    "* [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/)\n",
    "* [mechanize](https://pypi.python.org/pypi/mechanize) \n",
    "\n",
    "### and Chrome DevCenter & Plugins...\n",
    "\n",
    "* [XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=zh-CN)\n",
    "* [Postman](https://chrome.google.com/webstore/detail/postman/fhbjgbiflinjbdggehcddcbncdddomop?hl=zh-CN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Parsing technology\n",
    "- XPath / CSS Path\n",
    "- Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HTML and the DOM\n",
    "========\n",
    "The general idea behind web scraping is to retrieve data that exists on a website, and convert it into a format that is usable for analysis. Webpages are rendered by the brower from HTML and CSS code, but much of the information included in the HTML underlying any website is not interesting to us.\n",
    "We begin by reading in the source code for a given web page and creating a Beautiful Soup object with the BeautifulSoup function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "url = 'http://www.bazhuayu.com/about'\n",
    "r = urllib.urlopen(url).read()\n",
    "soup = BeautifulSoup(r)\n",
    "print type(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The soup object contains all of the HTML in the original document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print soup.prettify()[0:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HTML tags contained in the angled brackets provide structural information (and sometimes formatting), which we probably don't care about in and of itself but is useful for selecting only the content relevant to our needs. \n",
    " \n",
    "Beautiful Soup is essentially a set of wrapper functions that make it simple to select common HTML elements.\n",
    "\n",
    "![dom tree](images/dom_tree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most modern browsers have a parser that reads in the HTML document, parses it into a DOM (Document Object Model) structure, and then renders the DOM structure.\n",
    "Much like HTTP, the DOM is an agreed-upon standard. \n",
    " \n",
    "The DOM is much more than what described here, but for our purposes, what is most important to understand is that the text is only one part of an HTML element, and we need to select it explicitly.\n",
    "\n",
    "![dom tree](images/dom_tree2.png)\n",
    "\n",
    "More about DOM Tree:\n",
    "- [DOM visualization](http://dok.github.io/dom-visualization/)\n",
    "- [Live DOM Viewer](https://software.hixie.ch/utilities/js/live-dom-viewer/)\n",
    "- [HTML DOM Tree](https://gojs.net/latest/samples/DOMTree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath & lxml:\n",
    "========\n",
    "`<div id=’idValue’ class=’classValue’ username='username'>\n",
    "    <a href='http://www.bupt.edu.cn' target='_blank'>BUPT Home</a>\n",
    "    <div>\n",
    "        <a href='#'>click here 1st</a>\n",
    "        <a href='#'>click here 2nd</a>\n",
    "        <a href='#'>click here 3rd</a>\n",
    "    </div>\n",
    "</div>`\n",
    "![XPath CSS Path Cheat Sheet](images/XpathCssPathCheatSheet.jpg)\n",
    "(http://axatrikx.com/xpath-css-path-cheat-sheet/)\n",
    "\n",
    "- [[XPath] XPath 与 lxml （一）XPath 术语](http://www.cnblogs.com/ifantastic/p/3863271.html)  \n",
    "- [[XPath] XPath 与 lxml （二）XPath 语法](http://www.cnblogs.com/ifantastic/p/3863415.html)  \n",
    "- [[XPath] XPath 与 lxml （三）XPath 坐标轴](http://www.cnblogs.com/ifantastic/p/3863808.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To the internet (with dev tools)\n",
    "#### [MTime](http://www.mtime.com/hotest/)\n",
    "After poking around the MTime homepage with the help of command + option + i (mac) and a Chrome plugin, [XPath Helper](https://chrome.google.com/webstore/detail/xpath-helper/hgimnogjllphhhkhlmebbmlgjoejdpjl?hl=zh-CN), we can see that all the movie titles can be grabbed with the xpath, \"//dl/dt/a/text()\". lxml lets us retrieve all the text that matches our xpath."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full XPath for movie name:\n",
    "/html/body[@id='bodyRegion']/div[@class='centent']/div[@class='mtimetip']/div[@class='mtipbox']/div[@class='mtipmid']/div[@class='mtiplist'][2]/div[@class='clearfix']/div[@class='picbox']/dl/dt/a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8\n",
    "from lxml import html \n",
    "\n",
    "x = html.parse('http://www.mtime.com/hotest/')\n",
    "titles = x.xpath(\"//dt/a/text()\")\n",
    "print \"We got %s titles. Here are the first 5:\" % len(titles)\n",
    "for title in titles[:5]:\n",
    "    print title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "full XPath for score:\n",
    "/html/body[@id='bodyRegion']/div[@class='centent']/div[@class='mtimetip']/div[@class='mtipbox']/div[@class='mtipmid']/div[@class='mtiplist'][7]/div[@class='clearfix']/div[@class='picbox']/div[@class='score']/strong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scores = x.xpath(\"//div[@class='score']/strong/text()\")\n",
    "for score in scores:\n",
    "    print score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = x.path('//div[@class='mtiplist']')\n",
    "for group in groups:\n",
    "    titles = group.xpath(\"//dt/a/text()\")\n",
    "    scores = group.xpath(\"//div[@class='score']/strong/text()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(1, 11):\n",
    "    actors = x.xpath(\"//div[%s]/div/div/dl/dd/ul/li[2]/a/text()\" % i)\n",
    "    print actors\n",
    "    for actor in actors:\n",
    "        print actor\n",
    "    print '-----'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping for Multi-pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This page only has ~20 titles on it. There's a \"More Stories\" button at the bottom of the page, which brings us to another, similarly structured page with new titles and another \"More Stories\" button. To get more examples, we'll repeat the process above in a loop with each successive iteration hitting the page pointed to by the \"More Stories\" button. In order to figure out what the link is, go do some more investigating! You can also just look at the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We'll use sleep to add some time in between requests\n",
    "# so that we're not bombarding Gawker's server too hard. \n",
    "from time import sleep\n",
    "\n",
    "# Now we'll fill this list of gawker titles by starting\n",
    "# at the landing page and following \"More Stories\" links\n",
    "titles = []\n",
    "base_url = 'http://www.mtime.com/hotest/{}'\n",
    "next_page = \"http://www.mtime.com/hotest/\"\n",
    "\n",
    "# These are the xpaths we determined from snooping \n",
    "next_button_xpath = \"//a[@id='key_nextpage']/@href\"\n",
    "headline_xpath = \"//div[@class='picbox']/dl/dt/a/text()\"\n",
    "\n",
    "while len(titles) < 50 and next_page:\n",
    "    dom = html.parse(next_page)\n",
    "    headlines = dom.xpath(headline_xpath)\n",
    "    print \"Retrieved {} titles from url: {}\".format(len(headlines), next_page)\n",
    "    titles += headlines\n",
    "    next_pages = dom.xpath(next_button_xpath)\n",
    "    if next_pages: \n",
    "        next_page = base_url.format(next_pages[0]) \n",
    "    else:\n",
    "        print \"No next button found\"\n",
    "        next_page = None\n",
    "    sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for title in titles[:15]:\n",
    "    print title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('mtime_titles.txt', 'wb') as out:\n",
    "    out.write('\\n'.join(titles).encode('utf-8'))\n",
    "with open('mtime_titles.txt') as f:\n",
    "    titles_ = f.readlines()\n",
    "    \n",
    "print \"Well, we got {} Hot Movies!\".format(len(titles_))\n",
    "for title in titles[:15]:\n",
    "    print title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a special example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y = html.parse('http://music.baidu.com/top/dayhot')\n",
    "next_page = y.xpath(\"//a[@class='page-navigator-next']/@href\")\n",
    "print next_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print [str(next_page[0]).strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ['http://music.baidu.com' + \n",
    "       str(next_page[0])\n",
    "           .replace('\\\\t', '')\n",
    "           .replace('\\\\n', '')\n",
    "           .replace('[\\'', '').replace('\\']', '')\n",
    "           .strip()\n",
    "      ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another xpath example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "url = 'https://book.douban.com/series/1163?page=11'\n",
    "page = requests.get(url)\n",
    "y = html.fromstring(page.content)\n",
    "stars = y.xpath(\"//div[@class='star clearfix']/*\")\n",
    "for star in stars:\n",
    "    print star.attrib['class'], star.text.strip() if star.attrib['class'] == 'rating_nums' or star.attrib['class'] == 'pl'  else ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [A Simple Crawler Example](https://github.com/gwulfs/bostonml/blob/master/scraping/scraping.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about regular expression?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "url = 'http://book.douban.com/series/1163?page=11'\n",
    "re_extract = re.compile('<a href=\"(.*?)\" title=\"(.*?)\"[\\S\\s]*?class=\"pub\">([\\S\\s]*?)<\\/div>')\n",
    "page = requests.get(url)\n",
    "item_match = re.findall(re_extract, page.content)\n",
    "if item_match:\n",
    "    for item_info in item_match:\n",
    "        print item_info\n",
    "        print item_info[0]\n",
    "        print item_info[1]\n",
    "        print item_info[2].strip(), '\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Regular Expression:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Python Regular Expression](images/pyre_ebb9ce1c-e5e8-4219-a8ae-7ee620d5f9f1.png)\n",
    "\n",
    "References:\n",
    "- [Regulex](https://jex.im/regulex/)\n",
    "- [regular expressions 101](https://regex101.com/)\n",
    "- [Regex Builder](https://regexbuilder.codeplex.com/)\n",
    "- [Python正则表达式指南](http://www.cnblogs.com/huxi/archive/2010/07/04/1771073.html)\n",
    "\n",
    "- [BeautifulSoup](http://www.crummy.com/software/BeautifulSoup/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python编码问题\n",
    "简而言之，Python 2.x里字符串有两种：str和Unicode  \n",
    "前者到后者要decode，后者到前者要encode,'utf-8'为例：  \n",
    "str.decode('utf-8') -> Unicode  \n",
    "str <- Unicode.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print \"Type of    '中文'   is %s\" % type('中文')\n",
    "print \"Type of   '中文'.decode('utf-8')   is %s\" % type('中文'.decode('utf-8')) \n",
    "print \"Type of   u '中文'   is %s\" % type(u'中文')\n",
    "print \"Type of   u'中文'.encode('utf-8')   is %s\" % type(u'中文'.encode('utf-8')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建议一、使用字符编码声明，并且同一工程中的所有源代码文件使用相同的字符编码声明"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding=utf-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建议二、抛弃str，全部使用unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test1 = 'présenter'\n",
    "test2 = u'汉字'\n",
    "print type(test1)\n",
    "test_unicode = test1.decode('utf-8')\n",
    "print type(test_unicode)\n",
    "print (\"%s+%s\" % (test_unicode, test2)).encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JS里转义字符串的处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "src_str = u\"\\\\u4e09\\\\u73af\\\\u4ee5\\\\u5185\"\n",
    "print src_str.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tar_str_decode = src_str.decode(\"unicode-escape\")\n",
    "print tar_str_decode.encode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tar_str_eval = eval(\"u\\\"\" + src_str + \"\\\"\")\n",
    "print tar_str_eval.encode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [Python字符编码详解](http://www.cnblogs.com/huxi/archive/2010/12/05/1897271.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Ajax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s start with the imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from lxml import html\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will use requests.get to retrieve the web page with our data, parse it using the html module:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "page = requests.get('http://bbs.byr.cn/board/Recommend?p=1&_uid=guest')\n",
    "\u001d",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```html\n",
    "<td class=\"title_9\"><a href=\"……\" class=\"\">【公告】北邮人论坛热点活动管理条例</a></td>\n",
    "…… \n",
    "<td class=\"title_12\">| <a href=\"/user/query/wangxiaobupt\" class=\"c63f\">wangxiaobupt</a></td>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing this we can create the correct XPath query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_xpath = '//td[@class=\"title_9\"]/a/text()'\n",
    "author_xpath = '//a[@class=\"c63f\"]/text()'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the lxml xpath function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titles = tree.xpath(title_xpath)\n",
    "authors = tree.xpath(author_xpath)\n",
    "print \"We got %s titles and %s authors\" % (len(titles), len(authors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why? Let's introduce Ajax(Asynchronous JavaScript and XML)\n",
    "![Ajax](images/dojo_0401.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's review the page code... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "headers = {'X-Requested-With': ' XMLHttpRequest'}\n",
    "page = requests.get('http://bbs.byr.cn/board/Recommend?p=1&_uid=guest', headers=headers)\n",
    "\u001d",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Ajax example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page = requests.get('http://list.jd.com/list.html?cat=9987,653,655', headers=headers)\n",
    "tree = html.fromstring(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jd_names = tree.xpath(\"//div[@class='p-name']/a/em/text()\")\n",
    "for jd_name in jd_names[:5]:\n",
    "    print jd_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jd_ids = tree.xpath(\"//div[@class='gl-i-wrap j-sku-item']/@data-sku\")\n",
    "# for jd_id in jd_ids:\n",
    "#     print jd_id.attrib['data-sku']\n",
    "for ids_groups in [jd_ids[i:i+5] for i in range(0,len(jd_ids),5)]:\n",
    "    skuIds = '%2C'.join(map(lambda x: 'J_%s' % x, [ids_group for ids_group in ids_groups]))\n",
    "    page = requests.get('http://p.3.cn/prices/mgets?callback=jQuery8870889&type=1&area=1&skuIds=%s' % skuIds)\n",
    "    print page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JSON (JavaScript Object Notation)\n",
    "![JSON](./images/JSON.gif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Used for REST(Representational State Transfer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![rest](images/How to parse JSON In Java.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JSON in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "json_str = \"\"\"\n",
    "{\n",
    "  \"maps\": [\n",
    "    {\n",
    "      \"id\": \"blabla\",\n",
    "      \"iscategorical\": \"0\"\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"blabla\",\n",
    "      \"iscategorical\": \"0\"\n",
    "    }\n",
    "  ],\n",
    "  \"masks\": {\n",
    "    \"id\": \"valore\"\n",
    "  },\n",
    "  \"om_points\": \"value\",\n",
    "  \"parameters\": {\n",
    "    \"id\": \"valore\"\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "json_obj = json.loads(json_str)\n",
    "print json_obj, json_obj['masks']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refereneces:\n",
    "- [Introducing JSON](http://www.json.org/)\n",
    "- [Python: json — JSON encoder and decoder](https://docs.python.org/2/library/json.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Debugging Proxy\n",
    "- [Fiddler](http://www.telerik.com/fiddler)\n",
    "- [Charles](http://www.charlesproxy.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headers\n",
    "- User-Agent\n",
    "- Referer\n",
    "- Cookie\n",
    "- Accept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robot exclusion\n",
    "![Robot exclusion](images/robot_explained.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#http://www.intel.com/robots.txt\n",
    "\n",
    "# robots.txt exclusion for www.intel.com/ - US\n",
    "User-agent: *\n",
    "Disallow: /cgi\n",
    "Disallow: /iaweb/\n",
    "Disallow: /cpc/vision/\n",
    "Disallow: /intel/june297/\n",
    "Disallow: /cpc/eps/\n",
    "Disallow: /design/june297/\n",
    "Disallow: /cpc/archive/\n",
    "......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evolution of freshness and age in Web crawling\n",
    "![](images/freshness.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When the scale of Crawler grows..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical anatomy of a large-scale crawler\n",
    "![Typical anatomy of a large-scale crawler](images/5396ee05gw1ewdwnihf6jj20kl0hlwhx.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:  \n",
    "- [Python web scraping resource](http://jakeaustwick.me/python-web-scraping-resource/)\n",
    "- [Web Scraping 101 with Python](http://www.gregreda.com/2013/03/03/web-scraping-101-with-python/)\n",
    "- The Science of Crawl [Part 1](http://blog.urx.com/urx-blog/2014/9/4/the-science-of-crawl-part-1-deduplication-of-web-content) [Part2](http://blog.urx.com/urx-blog/2014/10/23/the-science-of-crawl-part-2-content-freshness)\n",
    "- [阅读 coursera-dl 源码](http://blog.onlyice.net/2015/07/11/read-coursera-dl-code/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler Tech Graph\n",
    "![crawler](images/crawler.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.sc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized DNS component\n",
    "![DNS](images/dns-rev-1.gif)\n",
    "![DNS Lookup Time](images/Waterfall-Chart-New.png)\n",
    "- Custom client for address resolution\n",
    "- **Caching** server\n",
    "- **Prefetching** client\n",
    "\n",
    "DNS cache poisoning:\n",
    "![DNS cache poisoning](images/dns-security-fig3.png)\n",
    "\n",
    "References:\n",
    "- [Dnsmasq](http://www.thekelleys.org.uk/dnsmasq/doc.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Thread\n",
    "![Multi-Thread](images/Picture1.png)\n",
    "\n",
    "### Thread Pool\n",
    "![thread pool](images/400px-Thread_pool.svg.png)\n",
    "\n",
    "#### Advantages of a Multithreaded\n",
    "- Improved performance and concurrency\n",
    "- Simplified coding of remote procedure calls and conversations\n",
    "- Simultaneous access to multiple applications\n",
    "- Reduced number of required servers\n",
    "\n",
    "#### Disadvantages of a Multithreaded\n",
    "- Difficulty of writing code\n",
    "- Difficulty of debugging\n",
    "- Difficulty of managing concurrency\n",
    "- Difficulty of testing\n",
    "- Difficulty of porting existing code\n",
    "\n",
    "### [Tomorrow](https://github.com/madisonmay/Tomorrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'http://google.com',\n",
    "    'http://facebook.com',\n",
    "    'http://youtube.com',\n",
    "    'http://baidu.com',\n",
    "    'http://yahoo.com',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "def download(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    start = time.time()\n",
    "    responses = [download(url) for url in urls]\n",
    "    html = [response.text for response in responses]\n",
    "    end = time.time()\n",
    "    print \"Time: %f seconds\" % (end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "\n",
    "from tomorrow import threads\n",
    "\n",
    "@threads(5)\n",
    "def download(url):\n",
    "    return requests.get(url)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    responses = [download(url) for url in urls]\n",
    "    html = [response.text for response in responses]\n",
    "    end = time.time()\n",
    "    print \"Time: %f seconds\" % (end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Sockets\n",
    "![sockets](images/TCP_IP_socket_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About APIs\n",
    "---\n",
    "Application Programming Interface(API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Long URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl \"http://api.longurl.org/v2/expand?url=http://kck.st/1Q51X6T&title=1&content-type=1&rel-canonical=1&format=json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "short_url = \"http://kck.st/1Q51X6T\"\n",
    "url_template = \"http://api.longurl.org/v2/expand?url=%s&title=1&content-type=1&rel-canonical=1&format=json\"\n",
    "tar_url = url_template % short_url\n",
    "print tar_url\n",
    "\n",
    "result = requests.get(tar_url).content\n",
    "json_result = json.loads(result)\n",
    "print \"short url:%s \\n->long url:%s \" % (short_url, json_result['long-url'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Page Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!curl 'http://clipped.me/algorithm/clippedapi.php?url=http://kck.st/1Q51X6T'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://kck.st/1Q51X6T\"\n",
    "url_template = \"http://clipped.me/algorithm/clippedapi.php?url=%s\"\n",
    "tar_url = url_template % url\n",
    "\n",
    "result = requests.get(tar_url).content\n",
    "json_result = json.loads(result)\n",
    "print \"for:%s\\ntitle:%s\\nsummary:\" % (url, json_result['title'])\n",
    "for summary in json_result['summary']:\n",
    "    print summary\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Share Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!curl 'http://free.sharedcount.com/?url=https://google.com/&apikey=d730c518430eabcabc46ab79528c744067afa17e'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://google.com\"\n",
    "url_template = \"http://free.sharedcount.com/?url=%s&apikey=d730c518430eabcabc46ab79528c744067afa17e\"\n",
    "tar_url = url_template % url\n",
    "\n",
    "result = requests.get(tar_url).content\n",
    "json_result = json.loads(result)\n",
    "print \"for %(url)s\\nFacebook:%(facebook)s\\nTwitter:%(twitter)s\" % {\n",
    "    'url' : url,\n",
    "    'facebook' : json_result['Facebook']['like_count'],\n",
    "    'twitter' : json_result['Twitter'],\n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![### About Deduplication](images/image5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    'http://www.google.com',\n",
    "    'http://www.aol.com',    \n",
    "    'http://www.google.com',\n",
    "]\n",
    "dedup = set()\n",
    "for url in urls:\n",
    "    dedup.add(url)\n",
    "print dedup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bloom Filter\n",
    "![Bloom Filter](images/800px-Bloom_filter.svg.png)\n",
    "![](images/Bloom-Filter-Simple-howto.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[pybloom](https://github.com/jaybaird/python-bloomfilter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pybloom import BloomFilter\n",
    "f = BloomFilter(capacity=1000, error_rate=0.001)\n",
    "[f.add(x) for x in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all([(x in f) for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message Queue\n",
    "![Message Queue](images/IC709523.png)\n",
    "- A sender can post a message to the queue.\n",
    "- A receiver can retrieve a message from the queue (the message is removed from the queue).\n",
    "- A receiver can examine (or peek) the next available message in the queue (the message is not removed from the queue).\n",
    "#### Scenarios for Asynchronous Messaging\n",
    "- Load balancing\n",
    "- Decoupling workloads\n",
    "- Temporal decoupling\n",
    "- Load leveling\n",
    "- Cross-platform integration\n",
    "- Asynchronous workflow\n",
    "- Deferred processing\n",
    "- Reliable messaging\n",
    "- Resilient message handling\n",
    "- Non-blocking receivers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google File System\n",
    "![GFS Architecture](images/gfs_architecture.png)  \n",
    "- Client translates file name and byte offset to chunk index.\n",
    "- Sends request to master.\n",
    "- Master replies with chunk handle and location of replicas.\n",
    "- Client caches this info.\n",
    "- Sends request to a close replica, specifying chunk handle and byte range.\n",
    "- Requests to master are typically buffered.\n",
    "#### GFS's winning attributes\n",
    "- Availability\n",
    "- Performance\n",
    "- Management\n",
    "- Cost   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Cookie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requests —— Persistent Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url1 = 'http://elib.cnki.net/grid2008/brief/result.aspx?DbPrefix=hotspotcomp&showTitle=学科学术热点'\n",
    "url2 = 'http://elib.cnki.net/request/search.aspx?action=&PageName=ASP.brief_result_aspx&DBViewType=FullText&DbCatalog=中国学术文献网络出版总库&DbPrefix=hotspotcomp&ConfigFile=hotspotcomp.xml&db_value=SUBJECT_BASE_INFO&NaviField=主题学科代码&orderby=(主题热度值,\\'integer\\')&txt_extension=false&his=1&SourceTypeCode=undefined&pSourceTypeCode='\n",
    "url3 = 'http://elib.cnki.net/DataCenter/DoGridTable.aspx?action=grid&pagename=ASP.brief_result_aspx&dbPrefix=hotspotcomp&dbCatalog=中国学术文献网络出版总库&ConfigFile=hotspotcomp.xml&sKuakuID=1&loadgroup=1&prio=true&db_value=SUBJECT_BASE_INFO'\n",
    "\n",
    "s = requests.Session()\n",
    "page = s.get(url1)\n",
    "page = s.get(url2)\n",
    "page = s.get(url3)\n",
    "print page.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pyCookieCheat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyCookieCheat\n",
    "local_cookies = pyCookieCheat.chrome_cookies(profile='Profile 4', domain='baidu.com')\n",
    "local_cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "page_content = requests.get(url, cookies=cookies).content, proxies={\"http\": \"http://117.177.243.42:85/\",}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crawler vs. Anti-Crawler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Crawler  | Anti-Crawler|\n",
    "|:------------- |:---------------|\n",
    "| Naive Crawler(No specific header)      | Verify User-Agent/Referer/X-Requested-With/... | \n",
    "| Multi-threaded      |  Connections per IP limit |\n",
    "| Multi-Proxy | Connections per IP limit + Proxy Dection | \n",
    "| *Multi-IP | Cookie Limit |\n",
    "| Cloud(PaaS(Platform-as-a-Service) / SaaS(Software-as-a-Service)) | OAuth/Cookie limit |\n",
    "| CrowdSourcing | Credit / Noise / HoneyPot / Pattern Recognition  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.sc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 核心问题 —— 真人/非人 判断\n",
    "- Multi-IP是业界主流做法——调动大量IP资源进行访问\n",
    "- 大量帐号的获取——自动/众包\n",
    "- 封堵 = 极限通告\n",
    "- 利用漏洞非长久之计\n",
    "- 最简单的HoneyPot——不可见但能解析/推断的链接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 数据无价，挖掘有价\n",
    "- 收藏无价，阅读有价"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Real Story......"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please stop automated bots against our platform and stealing data. This is not a good way for a smart guy to spend his time and energy. If you'd like to engage, we love to have intelligent guys join, work with us and make things better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> I'm so sorry to have troubled you. I'm not a data stealer, but a NLP researcher. I really love your service and I've recommended it to all my friends. In the last days, I did some data collection to analysis social relevances of them, sorry for any inconvenience this may have caused, and I'll stop the collection right now. Further more, do you have any plan to provide open(or commerial) API service? I would appreciate it very much."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We’re working on a set of OAuth2 APIs for trials with enterprise customers and they should be ready by the end of July. When they’re ready we’d be happy to share based on either a commercial agreement (if using for commercial purposes) or non-commercial agreement meant purely for research/education with all citations/references in place.\n",
    "\n",
    "> We’re happy to support research work that makes things better.\n",
    "\n",
    "> In the meantime, if you can please outline what exactly are you looking to do and how may we help, we can figure how we may get you the required data.\n",
    "\n",
    "> Once we understand and come to an agreement, we’ll unblock your primary account.\n",
    "Also, to reiterate, please refrain from all data collection without our knowledge and consent. You seem like a talented individual so we consider it mutually beneficial to maintain good relations with you.\n",
    "We appreciate you recommending us to your friends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "driver = webdriver.Firefox()\n",
    "\n",
    "driver.get('https://en.wikipedia.org/wiki/International_Space_Station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iss_table = driver.find_element_by_xpath('//table')\n",
    "iss_table_html = iss_table.get_attribute('outerHTML')\n",
    "\n",
    "print iss_table_html[:200]\n",
    "print '\\n. . .\\n'\n",
    "print iss_table_html[-200:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# for ipython notebook display\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(iss_table_html))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
